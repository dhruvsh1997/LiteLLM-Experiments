{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3IfmlLKT5oyK",
        "outputId": "dd2ccd44-8b55-4f7e-d249-57cd8a78e108"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting litellm\n",
            "  Downloading litellm-1.74.0-py3-none-any.whl.metadata (40 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/40.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.3/40.3 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp>=3.10 in /usr/local/lib/python3.11/dist-packages (from litellm) (3.11.15)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from litellm) (8.2.1)\n",
            "Requirement already satisfied: httpx>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from litellm) (0.28.1)\n",
            "Requirement already satisfied: importlib-metadata>=6.8.0 in /usr/local/lib/python3.11/dist-packages (from litellm) (8.7.0)\n",
            "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from litellm) (3.1.6)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from litellm) (4.24.0)\n",
            "Requirement already satisfied: openai>=1.68.2 in /usr/local/lib/python3.11/dist-packages (from litellm) (1.93.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from litellm) (2.11.7)\n",
            "Collecting python-dotenv>=0.2.0 (from litellm)\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from litellm) (0.9.0)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.11/dist-packages (from litellm) (0.21.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10->litellm) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10->litellm) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10->litellm) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10->litellm) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10->litellm) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10->litellm) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10->litellm) (1.20.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->litellm) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->litellm) (2025.6.15)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->litellm) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->litellm) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.23.0->litellm) (0.16.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=6.8.0->litellm) (3.23.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2<4.0.0,>=3.1.2->litellm) (3.0.2)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (0.26.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.68.2->litellm) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.68.2->litellm) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.68.2->litellm) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai>=1.68.2->litellm) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai>=1.68.2->litellm) (4.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.0->litellm) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.0->litellm) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.0->litellm) (0.4.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken>=0.7.0->litellm) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken>=0.7.0->litellm) (2.32.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers->litellm) (0.33.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm) (6.0.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken>=0.7.0->litellm) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken>=0.7.0->litellm) (2.4.0)\n",
            "Downloading litellm-1.74.0-py3-none-any.whl (8.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: python-dotenv, litellm\n",
            "Successfully installed litellm-1.74.0 python-dotenv-1.1.1\n"
          ]
        }
      ],
      "source": [
        "# Install the LiteLLM library, which lets us call different AI models (like OpenAI, Groq, Gemini) using one simple interface\n",
        "# We run this in Colab using !pip to install the package\n",
        "# LiteLLM is essential for this project because it simplifies API calls and lets us switch models easily\n",
        "!pip install litellm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries needed for the project\n",
        "import os        # Used to set environment variables (like API keys)\n",
        "import time     # Used to measure how fast each model responds\n",
        "from litellm import completion  # The main LiteLLM function to call AI models\n",
        "from google.colab import userdata  # Used to access API keys securely in Colab\n",
        "\n",
        "# Set up API keys securely using Colab Secrets\n",
        "# This avoids hardcoding sensitive keys in the code, keeping them safe\n",
        "# LiteLLM needs these keys to authenticate with OpenAI, Groq, and Gemini\n",
        "try:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY_')  # Get OpenAI key from Colab Secrets\n",
        "    os.environ[\"GROQ_API_KEY\"] = userdata.get('GROQ_API_KEY')      # Get Groq key\n",
        "    os.environ[\"GEMINI_API_KEY\"] = userdata.get('GOOGLE_API_KEY')  # Get Gemini key\n",
        "except Exception as e:\n",
        "    print(\"Error: Please set API keys in Colab Secrets (OPENAI_API_KEY, GROQ_API_KEY, GEMINI_API_KEY).\")\n",
        "    print(\"Go to the key icon in the sidebar, add each key, and rerun this cell.\")\n",
        "    raise e  # Stop execution if keys are missing\n",
        "\n",
        "# Print a confirmation to ensure keys are set\n",
        "print(\"API keys loaded successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFpfQqvd504v",
        "outputId": "eb4ba7f2-86bc-4e25-c64a-11ec48dc1d0c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API keys loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define simulated cow data to mimic real sensor data from a dairy farm\n",
        "# This data represents one cow's health and production metrics\n",
        "# In a real farm, this data would come from sensors (e.g., collars, milking machines)\n",
        "cow_data = {\n",
        "    \"cow_id\": \"COW123\",\n",
        "    \"rumination_time\": 400,      # Minutes/day the cow chews cud (normal: 450-550)\n",
        "    \"activity_level\": 500,       # Steps/day the cow takes (normal: 600-800)\n",
        "    \"milk_yield_yesterday\": 25,  # Liters of milk produced yesterday\n",
        "    \"udder_temperature\": 39.5,   # °C, udder temperature (normal: 38-39)\n",
        "    \"feed_intake\": 20            # kg/day of feed consumed\n",
        "}\n",
        "\n",
        "# Define the AI models we’ll test\n",
        "# LiteLLM lets us use these models with the same code, even though they’re from different providers\n",
        "models = [\n",
        "    \"openai/gpt-4o-mini\",              # OpenAI’s GPT-4o: great for detailed reasoning\n",
        "    \"groq/llama-3.3-70b-versatile\", # Groq’s LLaMA: fast for real-time predictions\n",
        "    \"gemini/gemini-1.5-flash\"     # Gemini: cost-effective, supports images (if needed later)\n",
        "]\n",
        "\n",
        "# Print the cow data to confirm what we’re analyzing\n",
        "print(\"Cow Data for Analysis:\")\n",
        "for key, value in cow_data.items():\n",
        "    print(f\"{key}: {value}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_Zz66Lg5_ei",
        "outputId": "6efc50d0-a93b-44ef-fcb5-70ff51c53684"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cow Data for Analysis:\n",
            "cow_id: COW123\n",
            "rumination_time: 400\n",
            "activity_level: 500\n",
            "milk_yield_yesterday: 25\n",
            "udder_temperature: 39.5\n",
            "feed_intake: 20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to detect cow health issues using LiteLLM\n",
        "# This sends the cow data to each AI model and asks it to identify problems like mastitis or lameness\n",
        "def detect_health_issues(cow_data, model):\n",
        "    # Create a detailed prompt to guide the AI\n",
        "    # The prompt includes the cow data, normal ranges, and instructions for a concise response\n",
        "    prompt = f\"\"\"\n",
        "    You are a dairy farm AI assistant with expertise in veterinary science.\n",
        "    Analyze the following cow data to identify potential health issues (e.g., mastitis, lameness).\n",
        "    Provide a one-sentence diagnosis and a one-sentence recommendation.\n",
        "    Use the following data:\n",
        "    - Cow ID: {cow_data['cow_id']}\n",
        "    - Rumination Time: {cow_data['rumination_time']} minutes/day (normal: 450-550)\n",
        "    - Activity Level: {cow_data['activity_level']} steps/day (normal: 600-800)\n",
        "    - Udder Temperature: {cow_data['udder_temperature']}°C (normal: 38-39)\n",
        "    - Milk Yield Yesterday: {cow_data['milk_yield_yesterday']} liters\n",
        "    - Feed Intake: {cow_data['feed_intake']} kg/day\n",
        "    Example response: \"The cow may have mastitis due to elevated udder temperature; recommend a veterinary exam.\"\n",
        "    \"\"\"\n",
        "    # Call the AI model using LiteLLM’s completion function\n",
        "    # temperature=0.3 ensures consistent, factual responses\n",
        "    # max_tokens=100 keeps the response short\n",
        "    try:\n",
        "        response = completion(\n",
        "            model=model,\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=0.3,\n",
        "            max_tokens=100\n",
        "        )\n",
        "        # Return the AI’s response (the diagnosis and recommendation)\n",
        "        return response.choices[0].message.content.strip()\n",
        "    except Exception as e:\n",
        "        # If the API call fails (e.g., rate limit, wrong key), return an error message\n",
        "        return f\"Error with {model}: {str(e)}\"\n",
        "\n",
        "# Test the function with one model to ensure it works\n",
        "test_model = models[0]  # Use the first model (GPT-4o)\n",
        "print(f\"Testing health diagnosis with {test_model}...\")\n",
        "health_result = detect_health_issues(cow_data, test_model)\n",
        "print(f\"Result: {health_result}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7FA5hWQ6HD4",
        "outputId": "21e52d2b-49e9-4be9-f66e-befc38c0aa65"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing health diagnosis with openai/gpt-4o-mini...\n",
            "Result: \"The cow may be experiencing health issues related to reduced rumination time and activity level, along with elevated udder temperature, indicating potential mastitis; recommend a veterinary exam and monitoring of her overall health.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to predict today’s milk yield using LiteLLM\n",
        "# This asks the AI to estimate milk production based on cow data\n",
        "def predict_milk_yield(cow_data, model):\n",
        "    # Create a prompt to predict milk yield\n",
        "    # It includes all cow data and asks for a prediction plus an explanation\n",
        "    prompt = f\"\"\"\n",
        "    You are a dairy farm AI assistant specializing in milk production.\n",
        "    Predict today's milk yield (in liters) for the cow based on the following data.\n",
        "    Provide a one-sentence prediction and a one-sentence explanation.\n",
        "    Use the following data:\n",
        "    - Cow ID: {cow_data['cow_id']}\n",
        "    - Rumination Time: {cow_data['rumination_time']} minutes/day\n",
        "    - Activity Level: {cow_data['activity_level']} steps/day\n",
        "    - Udder Temperature: {cow_data['udder_temperature']}°C\n",
        "    - Milk Yield Yesterday: {cow_data['milk_yield_yesterday']} liters\n",
        "    - Feed Intake: {cow_data['feed_intake']} kg/day\n",
        "    Example response: \"Predicted milk yield is 22 liters; lower rumination and high udder temperature suggest health stress.\"\n",
        "    \"\"\"\n",
        "    # Call the AI model using LiteLLM\n",
        "    # temperature=0.5 allows slight variation in predictions\n",
        "    try:\n",
        "        response = completion(\n",
        "            model=model,\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=0.5,\n",
        "            max_tokens=100\n",
        "        )\n",
        "        return response.choices[0].message.content.strip()\n",
        "    except Exception as e:\n",
        "        return f\"Error with {model}: {str(e)}\"\n",
        "\n",
        "# Test the function with one model\n",
        "test_model = models[0]  # Use GPT-4o\n",
        "print(f\"Testing milk yield prediction with {test_model}...\")\n",
        "yield_result = predict_milk_yield(cow_data, test_model)\n",
        "print(f\"Result: {yield_result}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_lUjA7VJ6HAG",
        "outputId": "021c0b21-506a-449b-ac33-27ff35636d42"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing milk yield prediction with openai/gpt-4o-mini...\n",
            "Result: Predicted milk yield is 23 liters; the high rumination time and adequate feed intake indicate good health and potential for stable production.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Process the cow data using all models and compare their results\n",
        "# This cell runs both functions for each model and measures performance\n",
        "results = {}  # Store results for each model\n",
        "print(f\"Analyzing cow {cow_data['cow_id']} with all models...\\n\")\n",
        "\n",
        "# Loop through each model (OpenAI, Groq, Gemini)\n",
        "for model in models:\n",
        "    # Record the start time to measure how fast the model responds\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Get health diagnosis\n",
        "    health_diagnosis = detect_health_issues(cow_data, model)\n",
        "\n",
        "    # Get milk yield prediction\n",
        "    yield_prediction = predict_milk_yield(cow_data, model)\n",
        "\n",
        "    # Calculate time taken for both tasks\n",
        "    elapsed_time = time.time() - start_time\n",
        "\n",
        "    # Store results in a dictionary\n",
        "    results[model] = {\n",
        "        \"health_diagnosis\": health_diagnosis,\n",
        "        \"yield_prediction\": yield_prediction,\n",
        "        \"time_taken\": elapsed_time\n",
        "    }\n",
        "\n",
        "# Display results for all models\n",
        "print(\"Comparison of Model Results:\")\n",
        "for model, result in results.items():\n",
        "    print(f\"\\nModel: {model}\")\n",
        "    print(f\"Health Diagnosis: {result['health_diagnosis']}\")\n",
        "    print(f\"Milk Yield Prediction: {result['yield_prediction']}\")\n",
        "    print(f\"Time Taken: {result['time_taken']:.2f} seconds\")\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ESgHr8Jj6G9T",
        "outputId": "6bc17e0c-708f-40a1-b9b4-c130b48b6c7c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analyzing cow COW123 with all models...\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "Comparison of Model Results:\n",
            "\n",
            "Model: openai/gpt-4o-mini\n",
            "Health Diagnosis: The cow may be experiencing health issues related to mastitis due to elevated udder temperature and decreased rumination and activity levels; recommend a veterinary exam and monitoring of milk quality.\n",
            "Milk Yield Prediction: Predicted milk yield is 28 liters; high rumination time and adequate feed intake indicate good health and potential for increased milk production.\n",
            "Time Taken: 2.15 seconds\n",
            "--------------------------------------------------\n",
            "\n",
            "Model: groq/llama-3.3-70b-versatile\n",
            "Health Diagnosis: Error with groq/llama-3.3-70b-versatile: litellm.InternalServerError: InternalServerError: GroqException - {\"error\":{\"message\":\"Internal Server Error\",\"type\":\"internal_server_error\"}}\n",
            "\n",
            "Milk Yield Prediction: Error with groq/llama-3.3-70b-versatile: litellm.InternalServerError: InternalServerError: GroqException - {\"error\":{\"message\":\"Internal Server Error\",\"type\":\"internal_server_error\"}}\n",
            "\n",
            "Time Taken: 20.74 seconds\n",
            "--------------------------------------------------\n",
            "\n",
            "Model: gemini/gemini-1.5-flash\n",
            "Health Diagnosis: COW123 shows potential for both reduced feed intake and mild mastitis due to decreased rumination, activity, and slightly elevated udder temperature;  recommend close monitoring of vital signs, milk quality, and feed intake, with veterinary consultation if symptoms worsen.\n",
            "Milk Yield Prediction: Predicted milk yield is 23 liters;  slightly reduced rumination time and yesterday's yield suggest a minor dip in production, but other factors remain within normal ranges.\n",
            "Time Taken: 1.32 seconds\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the system with different cow data scenarios\n",
        "# This helps us see how the AI handles healthy vs. unhealthy cows\n",
        "test_cases = [\n",
        "    {\n",
        "        \"cow_id\": \"COW456\",\n",
        "        \"rumination_time\": 500,      # Normal\n",
        "        \"activity_level\": 700,       # Normal\n",
        "        \"milk_yield_yesterday\": 28,  # Good yield\n",
        "        \"udder_temperature\": 38.5,   # Normal\n",
        "        \"feed_intake\": 22            # Normal\n",
        "    },\n",
        "    {\n",
        "        \"cow_id\": \"COW789\",\n",
        "        \"rumination_time\": 300,      # Very low\n",
        "        \"activity_level\": 400,       # Low\n",
        "        \"milk_yield_yesterday\": 20,  # Low yield\n",
        "        \"udder_temperature\": 40.0,   # Very high\n",
        "        \"feed_intake\": 18            # Low\n",
        "    }\n",
        "]\n",
        "\n",
        "# Process each test case\n",
        "for test_data in test_cases:\n",
        "    print(f\"\\nTesting Cow {test_data['cow_id']}...\")\n",
        "    results = {}\n",
        "    for model in models:\n",
        "        start_time = time.time()\n",
        "        health_diagnosis = detect_health_issues(test_data, model)\n",
        "        yield_prediction = predict_milk_yield(test_data, model)\n",
        "        elapsed_time = time.time() - start_time\n",
        "        results[model] = {\n",
        "            \"health_diagnosis\": health_diagnosis,\n",
        "            \"yield_prediction\": yield_prediction,\n",
        "            \"time_taken\": elapsed_time\n",
        "        }\n",
        "\n",
        "    # Display results\n",
        "    print(f\"Results for Cow {test_data['cow_id']}:\")\n",
        "    for model, result in results.items():\n",
        "        print(f\"\\nModel: {model}\")\n",
        "        print(f\"Health Diagnosis: {result['health_diagnosis']}\")\n",
        "        print(f\"Milk Yield Prediction: {result['yield_prediction']}\")\n",
        "        print(f\"Time Taken: {result['time_taken']:.2f} seconds\")\n",
        "        print(\"-\" * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YG9tKyWR6G63",
        "outputId": "c2b3b51b-a9e0-4081-c304-4c410227b7e1"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Testing Cow COW456...\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "Results for Cow COW456:\n",
            "\n",
            "Model: openai/gpt-4o-mini\n",
            "Health Diagnosis: The cow shows no immediate signs of health issues as all parameters are within normal ranges; however, continue to monitor for any changes in milk yield or behavior, and ensure regular veterinary check-ups.\n",
            "Milk Yield Prediction: Predicted milk yield is 30 liters; the high rumination time and adequate feed intake indicate good digestive health, likely contributing to increased milk production.\n",
            "Time Taken: 3.54 seconds\n",
            "--------------------------------------------------\n",
            "\n",
            "Model: groq/llama-3.3-70b-versatile\n",
            "Health Diagnosis: Error with groq/llama-3.3-70b-versatile: litellm.InternalServerError: InternalServerError: GroqException - {\"error\":{\"message\":\"Internal Server Error\",\"type\":\"internal_server_error\"}}\n",
            "\n",
            "Milk Yield Prediction: Predicted milk yield for COW456 is 27 liters; the cow's relatively high rumination time and moderate feed intake suggest a stable digestive system, but the slightly elevated udder temperature may indicate a minor health issue that could impact overall milk production.\n",
            "Time Taken: 13.82 seconds\n",
            "--------------------------------------------------\n",
            "\n",
            "Model: gemini/gemini-1.5-flash\n",
            "Health Diagnosis: Cow 456 shows no significant health concerns based on the provided data; continue routine monitoring.\n",
            "Milk Yield Prediction: Predicted milk yield is 26 liters;  slightly reduced rumination time and yesterday's yield suggest a minor dip in production, but overall parameters remain within a healthy range.\n",
            "Time Taken: 0.98 seconds\n",
            "--------------------------------------------------\n",
            "\n",
            "Testing Cow COW789...\n",
            "Results for Cow COW789:\n",
            "\n",
            "Model: openai/gpt-4o-mini\n",
            "Health Diagnosis: The cow may be experiencing mastitis due to elevated udder temperature and reduced rumination and activity levels; recommend a veterinary exam and possible treatment.\n",
            "Milk Yield Prediction: Predicted milk yield is 21 liters; the high rumination time and consistent feed intake indicate good health and productivity, despite the slightly elevated udder temperature.\n",
            "Time Taken: 1.74 seconds\n",
            "--------------------------------------------------\n",
            "\n",
            "Model: groq/llama-3.3-70b-versatile\n",
            "Health Diagnosis: The cow COW789 may be experiencing subclinical ruminal acidosis or other health issues due to decreased rumination time and activity level, along with elevated udder temperature; recommend a thorough veterinary examination, including a physical assessment and review of dietary and management practices to identify and address potential underlying causes.\n",
            "Milk Yield Prediction: Predicted milk yield is 21 liters; the combination of moderate rumination time, relatively low activity level, and high udder temperature, along with a significant feed intake, suggests a stable yet slightly reduced milk production for cow COW789 compared to yesterday's yield.\n",
            "Time Taken: 1.11 seconds\n",
            "--------------------------------------------------\n",
            "\n",
            "Model: gemini/gemini-1.5-flash\n",
            "Health Diagnosis: Cow 789 may be experiencing a combination of decreased feed intake and reduced activity leading to potential rumen dysfunction and possibly early-stage lameness; recommend close monitoring of feed intake, activity levels, and a veterinary assessment to rule out underlying issues.\n",
            "Milk Yield Prediction: Predicted milk yield is 18 liters; reduced feed intake and slightly elevated udder temperature suggest a potential decrease in milk production.\n",
            "Time Taken: 1.12 seconds\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FHXGzfbM6G4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1y_IDYtQ6G0r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}