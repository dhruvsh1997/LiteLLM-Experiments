# ğŸ§  LiteLLM Learning Repository

*A comprehensive exploration of LiteLLM's capabilities through diverse AI experiments*

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![LiteLLM](https://img.shields.io/badge/LiteLLM-Latest-green.svg)](https://github.com/BerriAI/litellm)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/)

## ğŸ“‹ Table of Contents

- [ğŸ¯ Project Overview](#project-overview)
- [ğŸ” Understanding LiteLLM](#understanding-litellm)
- [ğŸ—ï¸ Repository Architecture](#repository-architecture)
- [ğŸ§ª Current Experiments](#current-experiments)
- [ğŸ“Š Experiment Results](#experiment-results)
- [ğŸš€ Future Experiments](#future-experiments)
- [âš¡ Quick Start](#quick-start)
- [ğŸ“ˆ Performance Metrics](#performance-metrics)
- [ğŸ”§ Advanced Usage](#advanced-usage)
- [ğŸ“š Learning Resources](#learning-resources)

---

## ğŸ¯ Project Overview

This repository is a learning platform for **LiteLLM**, designed to explore its capabilities across various AI/ML scenarios. Our goal is to understand LiteLLM's **limits, strengths, and optimal use cases** through practical experiments.

### ğŸ¯ Learning Objectives

- Master LiteLLM's unified API approach
- Compare performance across different model providers
- Analyze cost-effectiveness of various AI models
- Explore RAG (Retrieval-Augmented Generation) implementations
- Develop production-ready AI solutions

---

## ğŸ” Understanding LiteLLM

### What is LiteLLM?

LiteLLM is a **unified interface** that simplifies interactions with multiple Large Language Model (LLM) providers through a consistent API, acting as a **universal translator** for AI models.

### LiteLLM's Position in the GenAI Ecosystem

```mermaid
graph TB
    subgraph "Developer Applications"
        A[Your Application]
        B[Chatbots]
        C[AI Analytics]
        D[RAG Systems]
        E[Content Generation]
    end
    
    subgraph "LiteLLM Layer"
        F[LiteLLM Unified API]
        G[Model Routing]
        H[Fallback Logic]
        I[Cost Optimization]
        J[Response Caching]
    end
    
    subgraph "AI Model Providers"
        K[OpenAI GPT-4]
        L[Anthropic Claude]
        M[Google Gemini]
        N[Groq LLaMA]
        O[Hugging Face]
        P[Local Models]
    end
    
    A --> F
    B --> F
    C --> F
    D --> F
    E --> F
    
    F --> G
    F --> H
    F --> I
    F --> J
    
    G --> K
    G --> L
    G --> M
    G --> N
    G --> O
    G --> P
    
    style F fill:#4CAF50,stroke:#2E7D32,stroke-width:3px
    style G fill:#2196F3,stroke:#1565C0,stroke-width:2px
```

### Key Benefits of LiteLLM

| Feature | Description | Impact |
|---------|-------------|--------|
| **ğŸ”„ Unified API** | One interface for all providers | Reduces integration complexity by 80% |
| **ğŸ’° Cost Optimization** | Automatic model routing based on cost | Saves 30-50% on API costs |
| **ğŸ›¡ï¸ Reliability** | Built-in fallback mechanisms | 99.9% uptime with redundancy |
| **ğŸ“Š Observability** | Built-in logging and monitoring | Complete request/response tracking |
| **âš¡ Performance** | Response caching and optimization | 40% faster response times |

---

## ğŸ—ï¸ Repository Architecture

```mermaid
graph LR
    subgraph "Repository Structure"
        A[ğŸ“ Experiment 1: Multi-API]
        B[ğŸ“ Experiment 2: RAGAS]
        C[ğŸ“ Experiment 3: Standard RAG]
        D[ğŸ“ Experiment 4: RAGAS with RAG]
        E[ğŸ“ Experiment 5: MLFlow with Corrective RAG]
        F[ğŸ“ utils/]
        G[ğŸ“ docs/]
    end
    
    subgraph "Experiment Types"
        H[ğŸ”¬ API Comparison]
        I[ğŸ“Š RAG Evaluation]
        J[ğŸ’¬ Document Q&A]
        K[ğŸ¯ Performance Analysis]
        L[ğŸ› ï¸ MLFlow Integration]
    end
    
    A --> H
    B --> I
    C --> J
    D --> I
    E --> L
    F --> K
    
    style A fill:#E3F2FD,stroke:#1976D2
    style B fill:#FFF3E0,stroke:#F57C00
    style C fill:#E8F5E8,stroke:#2E7D32
    style D fill:#FFF3E0,stroke:#F57C00
    style E fill:#FCE4EC,stroke:#C2185B
```

### ğŸ“‚ Current Experiments

```
litellm-learning/
â”œâ”€â”€ ğŸ§ª LiteLLM_Experiment1_MultipleAPI_LLM.ipynb     # Multi-provider comparison
â”œâ”€â”€ ğŸ“Š LiteLLM-Experiment2_RAGAS_RainNetwork.ipynb  # RAG evaluation with RAGAS
â”œâ”€â”€ ğŸ’¬ LiteLLM-Experiment3_Standard_RAG.ipynb       # Standard RAG implementation
â”œâ”€â”€ ğŸ“Š OutputParser/LiteLLM-Experiment4_RAGAS_with_RAG.ipynb  # RAGAS with RAG implementation
â”œâ”€â”€ ğŸ› ï¸ OutputParser/LiteLLM-Experiment5_MLFlow_with_CorrectiveRAG.ipynb  # MLFlow with Corrective RAG
â”œâ”€â”€ ğŸ“ utils/                                       # Utility scripts
â””â”€â”€ ğŸ“„ README.md                                    # This file
```

---

## ğŸ§ª Current Experiments

### ğŸšœ Experiment 1: Multi-API LLM Comparison (Dairy Farm AI)

**Objective**: Compare LiteLLM's performance across different providers for agricultural AI applications.

```mermaid
flowchart TD
    A[ğŸ„ Cow Health Data] --> B[ğŸ“Š Data Processing]
    B --> C{ğŸ¤– LiteLLM Router}
    
    C --> D[ğŸ”µ OpenAI GPT-4o]
    C --> E[ğŸŸ¢ Groq LLaMA 3.3]
    C --> F[ğŸŸ¡ Google Gemini 1.5]
    
    D --> G[ğŸ¥ Health Analysis]
    E --> G
    F --> G
    
    G --> H[ğŸ“ˆ Performance Comparison]
    H --> I[ğŸ“Š Results Dashboard]
    
    style C fill:#4CAF50,stroke:#2E7D32,stroke-width:3px
    style H fill:#2196F3,stroke:#1565C0,stroke-width:2px
```

**Key Features**:
- Multi-provider API testing
- Cost-performance analysis
- Agricultural domain application
- Real-time model comparison

---

### ğŸ“Š Experiment 2: RAGAS Evaluation Framework

**Objective**: Implement and evaluate RAG systems using RAGAS (Retrieval-Augmented Generation Assessment) framework.

```mermaid
flowchart TD
    A[ğŸ“„ Document Corpus] --> B[ğŸ” Vector Embedding]
    B --> C[ğŸ’¾ Vector Database]
    
    D[â“ User Query] --> E[ğŸ” Retrieval System]
    E --> C
    C --> F[ğŸ“‹ Retrieved Context]
    
    F --> G{ğŸ¤– LiteLLM Models}
    D --> G
    
    G --> H[ğŸ“ Generated Response]
    H --> I[ğŸ“Š RAGAS Evaluation]
    
    I --> J[ğŸ“ˆ Quality Metrics]
    J --> K[ğŸ¯ Performance Report]
    
    style G fill:#4CAF50,stroke:#2E7D32,stroke-width:3px
    style I fill:#FF9800,stroke:#F57C00,stroke-width:3px
    style J fill:#9C27B0,stroke:#7B1FA2,stroke-width:2px
```

**RAGAS Evaluation Metrics**:
- **Faithfulness**: Response accuracy to retrieved context
- **Answer Relevancy**: Response relevance to user query
- **Context Precision**: Relevance of retrieved context
- **Context Recall**: Completeness of retrieved information

**Key Features**:
- Automated RAG evaluation
- Multiple quality metrics
- Context-aware assessment
- Model comparison framework

---

### ğŸ’¬ Experiment 3: Standard RAG Implementation

**Objective**: Build a production-ready RAG system with document Q&A capabilities.

```mermaid
flowchart TD
    A[ğŸ“ Document Upload] --> B[ğŸ”¤ Text Extraction]
    B --> C[âœ‚ï¸ Text Chunking]
    C --> D[ğŸ” Embedding Generation]
    D --> E[ğŸ’¾ Vector Store]
    
    F[â“ User Question] --> G[ğŸ” Similarity Search]
    G --> E
    E --> H[ğŸ“‹ Top-K Results]
    
    H --> I{ğŸ¤– LiteLLM}
    F --> I
    
    I --> J[âœ¨ Contextualized Answer]
    J --> K[ğŸ‘¤ User Interface]
    
    style I fill:#4CAF50,stroke:#2E7D32,stroke-width:3px
    style E fill:#2196F3,stroke:#1565C0,stroke-width:2px
    style J fill:#FF9800,stroke:#F57C00,stroke-width:2px
```

**Key Features**:
- Document ingestion pipeline
- Semantic search capabilities
- Context-aware responses
- Interactive Q&A interface

---

### ğŸ“Š Experiment 4: RAGAS with RAG Implementation

**Objective**: Enhance RAG systems with advanced RAGAS evaluation for improved performance analysis.

**Key Features**:
- Advanced RAG evaluation
- Enhanced context retrieval
- Improved response quality metrics
- Integration with existing RAG systems

---

### ğŸ› ï¸ Experiment 5: MLFlow with Corrective RAG

**Objective**: Implement a Corrective RAG system integrated with MLFlow for experiment tracking and management.

**Key Features**:
- MLFlow experiment tracking
- Corrective RAG for improved accuracy
- Automated logging of metrics
- Scalable RAG pipeline

---

## ğŸ“Š Experiment Results

### ğŸ¯ Multi-API Performance Comparison

| Model | Avg Response Time | Accuracy Score | Cost per 1K Tokens | Reliability |
|-------|-------------------|----------------|-------------------|-------------|
| ğŸ”µ **OpenAI GPT-4o** | 1.45s | 95% | $0.015 | 99.9% |
| ğŸŸ¢ **Groq LLaMA 3.3** | 0.52s | 92% | $0.002 | 99.5% |
| ğŸŸ¡ **Gemini 1.5 Flash** | 0.78s | 94% | $0.001 | 99.7% |

### ğŸ“Š RAG System Evaluation

| Metric | Score | Description |
|--------|-------|-------------|
| **Faithfulness** | 0.85 | Response accuracy to context |
| **Answer Relevancy** | 0.78 | Query-response alignment |
| **Context Precision** | 0.82 | Retrieved context quality |
| **Context Recall** | 0.75 | Information completeness |

### ğŸ“ˆ Performance Metrics Dashboard

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Response Time Comparison â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                               â”‚
â”‚  OpenAI GPT-4o  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 1.45s â”‚
â”‚  Groq LLaMA 3.3 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 0.52s                    â”‚
â”‚  Gemini Flash   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 0.78s            â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ RAG Quality Metrics â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                               â”‚
â”‚  Faithfulness   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 0.85 â”‚
â”‚  Context Prec.  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 0.82 â”‚
â”‚  Answer Rel.    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 0.78     â”‚
â”‚  Context Rec.   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 0.75         â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ Future Experiments

### ğŸ“‹ Planned Experiment Roadmap

```mermaid
gantt
    title LiteLLM Learning Roadmap
    dateFormat  YYYY-MM-DD
    section Completed
    Multi-API Testing      :done, exp1, 2024-01-01, 2024-01-31
    RAGAS Evaluation       :done, exp2, 2024-02-01, 2024-02-28
    Standard RAG           :done, exp3, 2024-03-01, 2024-03-31
    RAGAS with RAG         :done, exp4, 2024-04-01, 2024-04-30
    MLFlow with Corrective RAG :done, exp5, 2024-05-01, 2024-05-31
    
    section Upcoming
    Production Deployment  :exp6, 2024-06-01, 2024-06-30
```

### ğŸ¯ Upcoming Experiments

| Experiment | Focus Area | Expected Insights |
|-----------|------------|-------------------|
| ğŸ”§ **Advanced RAG Systems** | Multi-modal RAG, agentic workflows | Complex document processing |
| ğŸ¯ **Fine-tuned Model Testing** | Custom model integration | Specialized task performance |
| ğŸ’² **Cost Optimization Suite** | Budget management strategies | ROI analysis frameworks |
| ğŸ­ **Production Scenarios** | Scalability and deployment | Enterprise-ready patterns |

---

## âš¡ Quick Start

### ğŸ› ï¸ Prerequisites

```bash
# Required versions
Python >= 3.8
pip >= 21.0
```

### ğŸ“¥ Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/litellm-learning.git
cd litellm-learning

# Install dependencies
pip install litellm ragas langchain chromadb sentence-transformers matplotlib pandas numpy mlflow
```

### ğŸ”‘ API Key Setup

#### Option 1: Google Colab Secrets (Recommended)

1. Open your Google Colab notebook
2. Click the **ğŸ”‘ key icon** in the left sidebar
3. Add these secrets:
   - `OPENAI_API_KEY`: Your OpenAI API key
   - `GROQ_API_KEY`: Your Groq API key
   - `GEMINI_API_KEY`: Your Google AI Studio key

#### Option 2: Environment Variables

```bash
export OPENAI_API_KEY="your-openai-key"
export GROQ_API_KEY="your-groq-key"
export GEMINI_API_KEY="your-gemini-key"
```

### ğŸš€ Running Experiments

#### Experiment 1: Multi-API Testing
```python
# Open LiteLLM_Experiment1_MultipleAPI_LLM.ipynb
# Run all cells to compare different API providers
```

#### Experiment 2: RAGAS Evaluation
```python
# Open LiteLLM-Experiment2_RAGAS_RainNetwork.ipynb
# Execute RAG evaluation with quality metrics
```

#### Experiment 3: Standard RAG
```python
# Open LiteLLM-Experiment3_Standard_RAG.ipynb
# Build and test document Q&A system
```

#### Experiment 4: RAGAS with RAG
```python
# Open OutputParser/LiteLLM-Experiment4_RAGAS_with_RAG.ipynb
# Execute enhanced RAG evaluation
```

#### Experiment 5: MLFlow with Corrective RAG
```python
# Open OutputParser/LiteLLM-Experiment5_MLFlow_with_CorrectiveRAG.ipynb
# Run MLFlow-integrated RAG experiment
```

---

## ğŸ“ˆ Performance Metrics

### ğŸ” Key Performance Indicators (KPIs)

```mermaid
graph LR
    subgraph "ğŸ“Š API Performance"
        A[â±ï¸ Response Time]
        B[ğŸ¯ Accuracy]
        C[ğŸ’² Cost Efficiency]
        D[ğŸ›¡ï¸ Reliability]
    end
    
    subgraph "ğŸ“Š RAG Quality"
        E[ğŸ¯ Faithfulness]
        F[ğŸ” Context Precision]
        G[ğŸ“ Answer Relevancy]
        H[ğŸ“‹ Context Recall]
    end
    
    style A fill:#4CAF50,stroke:#2E7D32
    style B fill:#2196F3,stroke:#1565C0
    style E fill:#FF9800,stroke:#F57C00
    style F fill:#9C27B0,stroke:#7B1FA2
```

### ğŸ“Š Evaluation Framework

Our experiments use comprehensive evaluation methods:

- **â±ï¸ Response Time**: Millisecond precision timing
- **ğŸ¯ Accuracy**: Task-specific scoring mechanisms
- **ğŸ’² Cost Analysis**: Real-time cost tracking
- **ğŸ“Š RAGAS Metrics**: Automated RAG quality assessment
- **ğŸ” Context Quality**: Retrieval effectiveness measurement

---

## ğŸ”§ Advanced Usage

### ğŸ”€ Model Routing Strategies

```python
# Example: Intelligent model routing for RAG
from litellm import completion

def smart_rag_routing(query_type, context_length):
    if context_length > 8000:
        model = "openai/gpt-4o"  # Large context handling
    elif query_type == "analytical":
        model = "openai/gpt-4o"  # Complex reasoning
    elif query_type == "factual":
        model = "groq/llama-3.3-70b-instruct"  # Fast responses
    else:
        model = "gemini/gemini-1.5-flash"  # Cost-effective
    
    return model
```

### ğŸ”§ RAG Configuration

```python
# Advanced RAG system configuration
from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings

# Vector store setup
embeddings = OpenAIEmbeddings()
vectorstore = Chroma(embedding_function=embeddings)

# RAG retrieval configuration
retriever = vectorstore.as_retriever(
    search_type="similarity",
    search_kwargs={"k": 5}
)
```

---

## ğŸ“š Learning Resources

### ğŸ“– Documentation & Guides

| Resource | Description | Link |
|----------|-------------|------|
| ğŸ“˜ **Official LiteLLM Docs** | Complete API reference | [litellm.ai](https://litellm.ai) |
| ğŸ“Š **RAGAS Documentation** | RAG evaluation framework | [ragas.io](https://ragas.io) |
| ğŸ“ **Our Experiment Notebooks** | Step-by-step implementations | Repository files |
| ğŸ’¡ **Best Practices** | Production-ready patterns | [/docs/best-practices/](./docs/best-practices/) |

### ğŸ¯ Learning Path

```mermaid
graph TD
    A[ğŸŒŸ Start with Experiment 1] --> B[ğŸ“Š Multi-API Comparison]
    B --> C[ğŸ” Learn RAG Basics]
    C --> D[ğŸ“Š RAGAS Evaluation]
    D --> E[ğŸ’¬ Standard RAG Build]
    E --> F[ğŸ¯ Advanced RAG Techniques]
    F --> G[ğŸ­ Production Deployment]
    
    style A fill:#4CAF50,stroke:#2E7D32,stroke-width:3px
    style G fill:#FF9800,stroke:#F57C00,stroke-width:3px
```

---

## ğŸ¤ Contributing

We welcome contributions! Here's how you can help:

### ğŸ¯ Ways to Contribute

- **ğŸ§ª Add New Experiments**: Share your LiteLLM use cases
- **ğŸ“Š Improve Evaluations**: Better metrics and analysis
- **ğŸ“ Documentation**: Tutorials and guides
- **ğŸ› Bug Reports**: Help us improve
- **ğŸ’¡ Feature Requests**: Suggest new experiments

### ğŸ“‹ Contribution Guidelines

1. **Fork** the repository
2. **Create** a feature branch
3. **Add** your experiment with documentation
4. **Test** thoroughly
5. **Submit** a pull request

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ¯ Goals & Vision

### ğŸ¯ Current Status

- âœ… Multi-API provider comparison
- âœ… RAGAS evaluation framework
- âœ… Standard RAG implementation
- âœ… RAGAS with RAG implementation
- âœ… MLFlow with Corrective RAG
- ğŸ”„ Advanced RAG techniques (in progress)

### ğŸš€ Vision

Building a comprehensive learning resource for LiteLLM that covers:
- API optimization strategies
- RAG system best practices
- Production deployment patterns
- Cost-effective AI solutions

---

**ğŸ’¡ Remember**: This repository is about learning and experimentation. Every experiment teaches us something new about LiteLLM's capabilities in different scenarios. Let's explore together!

---

<div align="center">

### ğŸŒŸ Happy Learning with LiteLLM! ğŸŒŸ

**[â­ Star this repo](https://github.com/yourusername/litellm-learning)** â€¢ **[ğŸ´ Fork it](https://github.com/yourusername/litellm-learning/fork)** â€¢ **[ğŸ“ Contribute](https://github.com/yourusername/litellm-learning/blob/main/CONTRIBUTING.md)**

</div>