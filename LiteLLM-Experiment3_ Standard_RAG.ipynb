{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMFeygnCYHvFVXU/pgNikdL"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jz2S2jSMoA4C","executionInfo":{"status":"ok","timestamp":1752324080558,"user_tz":-330,"elapsed":111361,"user":{"displayName":"Dhruv Sharma","userId":"05039836095185146849"}},"outputId":"afa102f2-45af-4cca-8b23-6a6cd2bb933b"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m81.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["# Install LangChain, LiteLLM, FAISS, and other dependencies\n","!pip install langchain langchain_community faiss-cpu pypdf tiktoken litellm sentence-transformers --quiet"]},{"cell_type":"code","source":["from langchain.document_loaders import PyPDFLoader\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","\n","# Load the uploaded PDF\n","pdf_path = \"/content/testPDF.pdf\"\n","loader = PyPDFLoader(pdf_path)\n","documents = loader.load()\n","\n","# Split text into smaller chunks\n","text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n","chunks = text_splitter.split_documents(documents)\n","\n","print(f\"Total chunks created: {len(chunks)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vB5olbxioLAd","executionInfo":{"status":"ok","timestamp":1752324132925,"user_tz":-330,"elapsed":50,"user":{"displayName":"Dhruv Sharma","userId":"05039836095185146849"}},"outputId":"42778405-2e6a-4f11-bad9-5c903c32f9c0"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Total chunks created: 3\n"]}]},{"cell_type":"code","source":["from langchain.embeddings import HuggingFaceEmbeddings\n","from langchain.vectorstores import FAISS\n","\n","# Load the SentenceTransformer model\n","embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L12-v2\")\n","\n","# Create FAISS vectorstore from document chunks\n","vectorstore = FAISS.from_documents(chunks, embedding_model)\n","\n","# Convert to retriever\n","# retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n","retriever = vectorstore.as_retriever(\n","    search_type=\"mmr\",  # Change search_type to \"mmr\"\n","    search_kwargs={\"k\": 3, \"lambda_mult\": 0.3} # Add lambda_mult here\n",")"],"metadata":{"id":"VB9VWwbHoT4N","executionInfo":{"status":"ok","timestamp":1752324274087,"user_tz":-330,"elapsed":2001,"user":{"displayName":"Dhruv Sharma","userId":"05039836095185146849"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["import litellm\n","from google.colab import userdata\n","# Set your GROQ API key here\n","litellm.api_key = userdata.get('GROQ_API_KEY')\n","\n","# Define a function to call Groq's llama3-70b-8192 using LiteLLM\n","def generate_answer_with_litellm(model_name, context, question):\n","    response = litellm.completion(\n","        model=model_name,\n","        messages=[\n","            {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers questions based on the company's employee handbook.\"},\n","            {\"role\": \"user\", \"content\": f\"Context:\\n{context}\\n\\nQuestion: {question}\"}\n","        ]\n","    )\n","    return response['choices'][0]['message']['content']"],"metadata":{"id":"iSKBkwDHqHR2","executionInfo":{"status":"ok","timestamp":1752324329486,"user_tz":-330,"elapsed":13538,"user":{"displayName":"Dhruv Sharma","userId":"05039836095185146849"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["# Function that retrieves context and generates answer using LLM\n","def rag_qa(question, model_name):\n","    # Step 1: Retrieve relevant context\n","    relevant_docs = retriever.get_relevant_documents(question)\n","    combined_context = \"\\n\".join([doc.page_content for doc in relevant_docs])\n","\n","    # Step 2: Generate answer\n","    answer = generate_answer_with_litellm(model_name, combined_context, question)\n","    return answer, combined_context"],"metadata":{"id":"u0i5C5b4qxmH","executionInfo":{"status":"ok","timestamp":1752324338806,"user_tz":-330,"elapsed":2,"user":{"displayName":"Dhruv Sharma","userId":"05039836095185146849"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["# LLaMA3 model name for Groq via LiteLLM\n","llama3_model_name = \"groq/llama-3.3-70b-versatile\"  # This is the LiteLLM model ID for LLaMA3-70B via Groq\n","\n","# Your test question\n","user_question = \"What is the GIT command for changing branch?\"\n","\n","# Generate answer\n","try:\n","    answer, context = rag_qa(user_question, llama3_model_name)\n","    print(\"=\"*80)\n","    print(f\"Answer from {llama3_model_name}:\\n\\n{answer}\")\n","except Exception as e:\n","    print(f\"Error: {e}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rJkDS0Nrq3K9","executionInfo":{"status":"ok","timestamp":1752324410416,"user_tz":-330,"elapsed":791,"user":{"displayName":"Dhruv Sharma","userId":"05039836095185146849"}},"outputId":"e98da963-6f42-4646-8d6e-b0ece13d2dbe"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-13-656141505.py:4: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n","  relevant_docs = retriever.get_relevant_documents(question)\n"]},{"output_type":"stream","name":"stdout","text":["================================================================================\n","Answer from groq/llama-3.3-70b-versatile:\n","\n","According to the company's employee handbook, the GIT command to change a branch is not explicitly mentioned in the provided text. However, in general Git terminology, the command to switch or change to a different branch is:\n","\n","`git checkout <branch-name>`\n","\n","or, alternatively, using the newer switch command:\n","\n","`git switch <branch-name>`\n","\n","Please note that the exact command may vary depending on the specific Git version or configuration used within the company. For the most accurate and up-to-date information, I recommend consulting the company's Git documentation or seeking guidance from the IT department.\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"b9zYcAYorIdV"},"execution_count":null,"outputs":[]}]}